import streamlit as st
import os
from openai import OpenAI

from utils.file_loader import load_file
from rag.chunking import chunk_text
from rag.embeddings import embed_texts
from rag.retriever import build_faiss_index, retrieve
from rag.prompts import build_prompt, plan_query


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="DocuMind",
    page_icon="ğŸ“„",
    layout="wide"
)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION STATE INITIALIZATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session_state():
    """Initialize all session state variables."""
    defaults = {
        "messages": [],
        "faiss_index": None,
        "metadata": [],
        "doc_names": [],
        "last_context": []
    }

    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


init_session_state()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DOCUMENT PROCESSING PIPELINE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_documents(uploaded_files):
    """
    Full RAG ingestion pipeline:
    1. Load files
    2. Chunk text
    3. Embed chunks
    4. Build FAISS index
    5. Store everything in session state
    """
    all_chunks = []
    doc_names = []

    for file in uploaded_files:
        pages = load_file(file)

        for text, page_num in pages:
            chunks = chunk_text(text, page_num, doc_name=file.name)
            all_chunks.extend(chunks)

        doc_names.append(file.name)

    if not all_chunks:
        return

    # Embed in batch
    texts_only = [chunk["text"] for chunk in all_chunks]
    embeddings = embed_texts(texts_only)

    # Build FAISS index
    index = build_faiss_index(embeddings)

    # Store in session state
    st.session_state.faiss_index = index
    st.session_state.metadata = all_chunks
    st.session_state.doc_names = doc_names


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG RESPONSE GENERATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_response(question: str) -> str:
    """Generate grounded answer using multi-query retrieval."""

    # Step 1 â€” Plan Retrieval Strategy
    plan = plan_query(question, client)
    sub_queries = plan.get("queries", [question])
    top_k = plan.get("top_k", 6)

    if question not in sub_queries:
        sub_queries.append(question)

    # Step 2 â€” Multi-Query Retrieval
    all_results = []

    for sub_query in sub_queries:
        query_embedding = embed_texts([sub_query])[0]

        results = retrieve(
            query_embedding,
            st.session_state.faiss_index,
            st.session_state.metadata,
            top_k=top_k
        )

        all_results.extend(results)

    # Step 3 â€” Deduplicate Results
    unique_chunks = {}
    for chunk in all_results:
        key = (chunk["text"], chunk["page"], chunk["doc_name"])
        unique_chunks[key] = chunk

    final_chunks = list(unique_chunks.values())
    st.session_state.last_context = final_chunks

    # Step 4 â€” Build Context
    context = "\n\n".join(
        f"[Page {r['page']} - {r['doc_name']}]\n{r['text']}"
        for r in final_chunks
    )

    # Step 5 â€” Build Final Prompt
    final_prompt = f"""
You are a document intelligence assistant.

Answer the user's question using ONLY the provided context.
If partial information exists, clearly explain conditions and limitations.
If information is missing, state that explicitly.

Context:
{context}

Question:
{question}
"""

    # Step 6 â€” Generate Answer
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You answer questions grounded strictly in provided document context."
            },
            {
                "role": "user",
                "content": final_prompt
            }
        ],
        temperature=0
    )

    return response.choices[0].message.content


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CENTRAL QUESTION HANDLER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_question(question: str):
    """Processes user or insight button queries."""
    st.session_state.messages.append(
        {"role": "user", "content": question}
    )

    with st.spinner("Thinking..."):
        answer = generate_response(question)

    st.session_state.messages.append(
        {"role": "assistant", "content": answer}
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI LAYOUT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“„ DocuMind")
st.caption("ğŸŸ¢ Session-based | Documents deleted on refresh")
st.caption("ğŸ§  RAG-powered document intelligence")
st.divider()

col1, col2 = st.columns([1, 2])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LEFT COLUMN â€” DOCUMENTS & INSIGHTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with col1:
    st.subheader("ğŸ“ Upload Documents")

    uploaded_files = st.file_uploader(
        "Upload your documents",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True,
        label_visibility="collapsed"
    )

    if uploaded_files:
        uploaded_names = [f.name for f in uploaded_files]

        if uploaded_names != st.session_state.doc_names:
            with st.spinner("Processing documents..."):
                process_documents(uploaded_files)
            st.success(f"âœ… {len(uploaded_files)} document(s) ready")

    if st.session_state.doc_names:
        st.markdown("**Loaded Documents:**")
        for name in st.session_state.doc_names:
            st.markdown(f"- ğŸ“„ {name}")

    st.divider()

    # Quick Insights
    st.subheader("âš¡ Quick Insights")
    st.caption("Requires a document to be uploaded first")

    INSIGHT_PROMPTS = {
        "ğŸ“ Summarize": "Summarize all uploaded documents clearly.",
        "ğŸ”‘ Key Points": "What are the key points?",
        "âš ï¸ Risks": "What are potential risks or concerns?",
        "âœ… Checklist": "Generate a checklist of action items."
    }

    for label, prompt in INSIGHT_PROMPTS.items():
        if st.button(label, use_container_width=True):
            if st.session_state.faiss_index is None:
                st.warning("Please upload a document first.")
            else:
                handle_question(prompt)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RIGHT COLUMN â€” CHAT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with col2:
    st.subheader("ğŸ’¬ Chat")

    # Scrollable native container
    chat_container = st.container(height=520, border=True)

    with chat_container:
        if not st.session_state.messages:
            st.info("Upload a document and ask a question to get started.")
        else:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

    question = st.chat_input("Ask a question about your documents...")
    if question:
        if st.session_state.faiss_index is None:
            st.warning("Please upload a document first.")
        else:
            handle_question(question)
            st.rerun()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RETRIEVED CONTEXT EXPANDER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.divider()

with st.expander("ğŸ” Retrieved Context â€” Last Query"):
    if st.session_state.last_context:
        for i, chunk in enumerate(st.session_state.last_context):
            st.markdown(
                f"**Chunk {i+1} â€” Page {chunk['page']} Â· {chunk['doc_name']}**"
            )
            st.text(chunk["text"])
            st.divider()
    else:
        st.write("Retrieved chunks will appear here after your first question.")

st.caption("ğŸ”’ Files are processed in-memory and not stored anywhere.")